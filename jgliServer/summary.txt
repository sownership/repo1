- 무조건적인 확장성보다는 변할 것과 변하지 않을 것을 잘 결정하는 것이 중요.

- 통신방식, 프로토콜, 요청처리방식, 처리로직들이 서로 의존성이 없도록 설계하여 결합도가 낮음
- systemin, socket 뿐 아니라 새로운 통신방식을 사용하는 서버도 지원하도록 설계
- 다양한 통신방식을 사용하는 클라이언트들을 동일하게 취급할 수 있도록 설계
- 통신방식이 같아도 다양한 프로토콜을 처리할 수 있도록 설계
- 인크립션 뿐 아니라 다른 형태의 요청도 처리할 수 있도록 함

- 파일/통신 입출력은 비동기로 설계하여 thread 자원 활용율을 높임
- 요청 command 에 따라 동적으로 의존성을 주입할 수 있도록 dependancyinjectionutil 을 사용함
  구현은 reflection 과 설정파일을 사용함
- 인크립션은 서로 결합해서 사용할 수 있음 즉, 서브클래싱을 피할 수 있는 구조로 되어 있음.
- 인크립션 방식이 다르다고 controller 가 추가되지 않도록 설계함.
  즉 하나의 controller 가 요청 명령에 따라 다양한 인크립터를 사용할 수 있음
- java nio.2 비동기 통신을 통해 적의수의 thread 로 많은 client 를 처리할 수 있다
- client 가 응답하지 않는 경우 timeout 처리를 하고 구현은 scheduledExecutorService 를 사용한다.
- controller 가 사용할 ThreadPool 은 cpu core 수 만큼만 사용한다.
  file, socket 처리가 모두 비동기이기 때문이다.
- 인크립터는 현재까지 읽은 파일에서 확실한 부분만 처리하고 나머지는 보관한다
- OOM 방지를 위해 파일은 한번에 8k 만큼만 읽는다.
	 * read -> Convert -> write 가 계속된다
	 * read 는 한번에 8k 이상하는 것은 더 이상의 성능향상이 없다. 오히려 메모리 부담을 줄 수 있다
	 * read 와 write 가 너무 느리므로 Convert 8k 를 multithread 로 하는 것은 전체 속도에 의미를 못준다
	 * 오히려 read 와 write 를 위한 thread 를 잡아두지 않는 것이 최선이다
	 * 즉 read 와 write 를 비동기로 처리한다
- 제안: 프로토콜 설계의 보완. 예를들어 인크립션방식 등이 client 로부터 전달.

https://www.concretepage.com/java/jdk7/example-seekablebytechannel-java-nio-channels



- 본 제품이 시장 경쟁력이 있으려면 성능이 좋고 유연해야 합니다.
그래서 이 두가지 관점에서 설계를 진행하였습니다.
성능이란 동시에 많은 클라언트를 서비스할 수 있고 동시에 많은 처리를 할 수 있는 것으로 정의하였습니다.
유연함이란 변경될 가능성이 있는 항목들에 대해 확장에 열려있고 변경에 닫혀있는 것으로 정의하였습니다.
모든 내용을 유연하게 만들려고 하면 복잡성이 높아지므로
여기서는 통신방법, 데이터포맷, 명령어셋, 압축/암호화 방법으로 정하였습니다.
다시 성능으로 돌아가 많은 클라이언트란 얼마일까요?
이 설계는 1만개 클라이언트로 정하였습니다. c10k problem 이라는 말이 있듯이
단일 노드가 10000개 클라이언트를 동시에 서비스할 수 있다면 경쟁력이 있다고 생각하였습니다.
초당 얼마나 많은 요청을 처리할 수 있으면 많다고 말할 수 있을까요?
저는 10k 클라이언트에서 10k 요청 처리라고 정의하였습니다.
설계 후 일부 기능을 실제 구현을 해서 회사 노트북으로 성능 테스트를 해 보았는데요.
10만개 클라이언트에 서비스를 할 수 있었고 초당 10k 클라이언트에서 10k 의 통신이 이루어집니다.
10만개 클라이언트일 경우 파일도 동시에 10만개 오픈하여 압축 및 암호화를 처리하는 것을 확인하였습니다.
회사 노트북에서 apache server 가 ?? 성능이 나오는데 단순비교는 맞지 않지만 이 정도면 의미있는 성능이라고
볼 수 있겠습니다.

서론이 길었는데요. 이제 유연성을 먼저 살펴보고 성능에 대해 말씀드리겠습니다.
유연성 있는 설계에서 가장 중요한 부분은 변할 가능성이 있는 부분과 없는 부분을 명확하게 결정하는 것입니다.
앞서 말씀드렸듯이 유연성의 포인트를 통신방식, 데이터포맷, 명령어셋 그리고 압축/암호화의 4개로 잡았습니다.
갑자기 udp 로 통신하는 클라이언트가 추가된다면 acceptor 모듈에서 udp accepter 클래스와
udp client 클래스만 추가하면 되고 나머지 모듈들은 변경이 없습니다.
json 데이터 포맷으로 통신이 필요하다면 decoder 모듈에서 json decoder 만 추가해 주면 됩니다.
명령어셋 및 압축/암호화 모듈도 같은 설명이 적용됩니다.
즉 유연성을 확보하기 위해 open-closed 원칙이 적용된 것입니다.

accepter 모듈은 systemioaccepter 와 niosocketaccepter 가 있고 이들은 absioacceptor 로 동일하게 취급됩니다.

이때 총 사용된 thread 수는 8 개입니다.
즉 모든 i/o 및 biz logic 을 비동기로 작성한 것입니다.
잘 아시다시피 이는 적합한 기술과 알고리즘을 적용하는 것 뿐 아니라 많은 경험이 필요한 부분입니다.
예를 들어 물밀듯이 밀려들어오는 connect 요청들을 어떻게 최대한 refuse 하지 않고 받아낼 것인지
송수신, 압축, 파일처리의 cpu 비중을 어떻게 할 것인지, 10만 클라이언트에 대해 OOM 이 발생하지 않는
상태로 어떻게 최선의 퍼포먼스를 낼지 등등에 대한 것들일 것입니다.
계속해서 적용한 기술에 대해 간단하게 살펴보겠습니다.
java nio.2 의 비동기 통신을 지원하는 class 들을 사용하였습니다.
대표적으로 asynchronousserversocketchannel 인데 이 accept, socket r/w 로 3개의 기능을 수행합니다.
client 입장에서 accept 를 빨리 처리 안해주면 exception 이 발생하므로 이 기능에 전체 쓰레드 8개 중 4개를
할당하였습니다.
또한 같은 이유로 accept 후 해야 할 일들은 이 4개가 아닌 biz threadpool 로 위임하였습니다.
biz threadpool 은 2개로 설정되어 있습니다. thread 가 2개밖에 없지만 모든 biz logic 도 비동기로
설계되어 초당 수천에서 수만개의 처리가 되는 것을 구현을 통해 확인하였습니다.
시간상 하위설계와 관련된 부분은 많이 적지 못했지만
여기서 bytebuffer 에 대해 dma 어로케이션을 사용하는 것 또한 중요합니다.
소켓 송수신과 뒤에 이야기할 파일입출력은 커널 메모리를 직접 사용해서 힙메모리 및 cpu 사용율을 절감할 수 있는
dma 어로케이션을 사용해야 합니다.
native code 와 관련된 메모리 사용이 없는 나머지 모듈들은 직접 힙메모리를 쓰는 것이 더 유리하겠습니다.

decoder 를 별도의 모듈로 둔 2가지 이유에 대해 말씀 드리겠습니다.
많은 클라이언트와 대량 통신을 하다보면 client 가 abc 라는 packet 을 전송해도
server 는 a 를 받고 잠시 후에 bc 를 받을 수 있습니다.
이렇게 연속으로 들어오는 byte들의 어디부터 어디까지가 의미있는 하나의 요청인지 디코딩을 해야 합니다.
만약 abc 중 ab 가 완결된 요청이고 c 는 다음 요청의 일부라면
decoder 는 ab 를 추출해서 돌려주고 c 는 가지고 있다가 다음 packet 과 연결해서 다시 decoding 을 하게 되는 것입니다.
두번째 이유는 data format 즉 data protocol 이 달라질 수 있기 때문에 확장성 측면에서 decode 모듈이 존재합니다.
예를 들어 json 기반의 protocol 도 있을 수 있고 xml 기반도 있을 수 있으며 이번 과제처럼 단순 word 기반의
data protocol 도 있을 수 있습니다. 즉 decoder 도 전략 패턴 형태로 사용되는 것입니다.
상황에 따라 디코더를 변경할 의도가 있으므로 전략패턴으로 볼 수 있고
억셉터에서 디코딩 추상층을 분리하려는 의도도 있으므로 브릿지 패턴을 적용했다고 볼 수도 있겠습니다.
실제 업무에서는 디코더뿐 아니라 인코더도 두고 메시지 오브젝트도 설계합니다.
지금은 지면상 그리고 시간상 생략하였습니다.

accepter 에서 client packet 을 받아서 decoder 로 하여금 완성된 요청을 찾아내게 하고
그 결과를 controller 모듈로 보내서 biz 로직을 처리하도록 합니다.
controller 모듈은 전형적인 front controller 패턴을 사용하였습니다.
front controller 에서는 요청의 command 에 따라 적절한 처리 controller 로 요청을 전달하게 됩니다.
front controller 와 처리 controller 는 결합도를 낮추기 위해 dependancy inversion principle 이 적용되는데
이를 위해 외부 파일에 의한 의존성 주입하는 방법으로 설계하였습니다.
즉 command 에 매핑되는 controller 가 mapper 설정파일에 존재하게 됩니다.
또한 이 controller 들은 모두 single instance 로 동작합니다.
이유는 많은 클라이언트들이 동시에 많은 요청을 할 때마다 controller 를 생성하지 않기 위해서 입니다.
즉 cpu, memory 사용율을 낮추기 위함이고 이는 was 의 servlet 과 같은 구조라고 생각하시면 되겠습니다.
계속 관련된 내용을 말씀드리겠지만 이 설계는 회사 노트북에서 10000개의 클라이언트가 동시 요청을 했을 때
cpu, mem, thread 등에 문제가 발생하지 않도록 계산된 설계입니다.
또한 이 controller 구조 설계는 향후 이 프로그램이 압축,암호화뿐 아니라 다양한 요청을 처리하도록
열린 아키텍처입니다.

ack 를 처리하는 controller 는 commu controller 가 파일의 다음 내용을 처리하도록 신호를 전달해야 합니다.
이 프로그램에서는 모듈간 결합도를 낮추기 위해 전체적으로 eventbus pattern 즉 pub/sub 를 사용합니다.
commu controller 는 파일 8k 를 읽은 후 sub 후 client 로 전송하고 client 는 ack 를 보내면
ack controller 는 pub 를 하게되고 commu controller 가 다음 처리를 하게 됩니다.
이때 8kb 에 대해서도 이야기를 해 보겠습니다.
NTFS 나 linux 의 recommand block size 는 4KB 이고 효율을 위해 보통 8KB 를 사용한다.
8KB 이하는 8KB 보다 성능이 떨어지고 8KB 이상은 메모리 소비 대비 효율이 높지 않습니다.
그리고 동시 10만 클라이언트를 지원하기 위해 8KB 로 설계를 진행하였습니다.
AsynchronousFileChannel 로 비동기 io 를 하고 싶으나
jdk 의 windows 구현체의 경우는 file마다 thread 를 생성하는 문제가 있어서
seekablefilechannel 과 executorservice 를 이용해서 모든 요청이 공평하게 파일 처리가 되도록 한다.
이 때 file r/w 와 converter 에는 cpu core 의 1/4~1/2만 할당하여 시스템 전체 성능을 유지해 준다.

마지막으로 컨버터에 대해 말씀드리겠습니다.
컨버터는 암축 및 암호화를 담당하는 모듈입니다.
controller 와 마찬가지로 command 에 따라 다른 converter 들을 사용할 수 있도록 전략패턴이고
mapper 설정파일로 di 가 적용되어 있습니다.
또한 여러 압축과 암호화를 한번에 적용할 수 있도록 decorator 패턴으로 설계되었습니다.
프로그램 플로우가 client connect 후 socket read -> file read -> convert -> socket write 가 반복되는 구조인데
client 와의 통신 및 file read 는 os kernel 성능에 의존적이고
client 수가 늘어날수록 단위시간당 통신 속도는 떨어지게 됩니다.
반면에 convert 는 개발자의 능력에 의존적이라고 볼 수 있습니다.
convert 튜닝 정도에 따라 프로그램의 수행속도는 몇배가 차이가 나게 됩니다.
결국 file 을 읽은 후 처리를 할 때 모든 byte 에 대해 convert 에서 사용하는 모든 자료구조는
log(1) 을 지향해야 합니다.
제가 실제 구현한 코드도 튜닝을 거쳐 log(1) 로 작성되었습니다.

프로토콜 프레임 설계는 매우 중요합니다. 이유를 설명 드리겠습니다.
프로토콜은 시작문자가 있어야 합니다. 유실이 덜되는 tcp 의 경우라도 결국 유실은 발생하기 때문입니다.
패킷의 길이가 있어야 합니다. 그래야 어디까지 처리해야 할 지 알 수 있습니다.
flow control 을 위해 통신번호도 꼭 있어야 합니다.
그래야만 서로 누락된 번호를 재요청할 수 있고 이 응답이 어떤 요청의 응답인지 알 수 있습니다.
checksum 또한 중요합니다. tcp 도 결국 유실이 될 수 있기 때문입니다.


